{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crash Web Scrape Notes (Python-BeautifulSoup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### =======Syllabus=======\n",
    "__1. Introduction to Web Scrape__\n",
    "\n",
    "__2. Python Libraries__\n",
    "\n",
    "__3. Checking The Status Of a Web Site__\n",
    "\n",
    "    3.1. Status Codes\n",
    "    3.2. Header Editing\n",
    "__4. Editing URLs__\n",
    "    \n",
    "    4.1. Adding Standard Parameters To a URL\n",
    "    4.2. Google Search URL Parameters\n",
    "    4.3. Adding Variable-Parameters To a URL\n",
    "    4.4. Finding The Parameters In URLs\n",
    "__5. Gathering Content__\n",
    "    \n",
    "    5.1. General Content\n",
    "    5.2. Specific Content\n",
    "    5.3. CAPSTONE-1\n",
    "    5.4. BONUS: Scrape Tables with Pandas\n",
    "\n",
    "__6. Some Other Usefull Information__\n",
    "    \n",
    "    6.1. Adding Cookies To Requests\n",
    "    6.2. Scraping via Proxies\n",
    "\n",
    "__7. CAPSTONE-2__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction to Web Scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Scraping is an efficient way to extract data from open sources (different websites). It is an automated process that;\n",
    "- reaching a web site (URL) by browser (ex:Selenium library for Python) or by HTTP directly (ex: BeautifulSoup library for Python)\n",
    "- surpassing the captcha or any other security precautions (if exists)\n",
    "- finding the relevant (wanted) data\n",
    "- gathering the data\n",
    "- saving the data in structured type (to a database, .CSV file..etc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/msklc/crash_web_scrape_notes/blob/master/images/web_scrape_schema.jpg?raw=true\">\n",
    "\n",
    "__WARNING:__\n",
    "\n",
    "__Web scraping itself canâ€™t be illegal. But before scraping any data, make sure to check the \"terms of services\". Otherwise, while scraping it will be possible to break the law and commit a crime.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 main issue for web scrape:\n",
    "- Editing the URL\n",
    "- Finding the content location for gathering \n",
    "\n",
    "<img src=\"https://github.com/msklc/crash_web_scrape_notes/blob/master/images/2main_issue.jpg?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Python Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__via directly HTTP__\n",
    "- [Requests](https://requests.readthedocs.io/en/master/)\n",
    "- [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- urllib\n",
    "- Scrapy\n",
    "- LXML\n",
    "- Pandas ??REALLY??\n",
    "\n",
    "__via Browser__\n",
    "- Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "print('requests version:', requests.__version__)\n",
    "print('BeautifulSoup version:', bs4.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Checking The Status Of a Web Site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3.1. Status Codes__\n",
    "- 200 : OK (Successfuly Connection)\n",
    "- 3xx : Redirection\n",
    "- 400 : Bad Request\n",
    "- 401 : Unauthorized\n",
    "- 403 : Forbidden\n",
    "- 404 : Not Found\n",
    "- 5xx Server Error\n",
    "    - 500 : Internal Server Error\n",
    "    - 501 : Not Implemented\n",
    "    - 502 : Bad Gateway\n",
    "    - 503 : Service Unavailable\n",
    "    - 504 : Gateway Timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url='http://www.google.com'\n",
    "r=requests.get(url)\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__More Example__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url_list=['http://www.deeploai.com', 'http://worldagnetwork.com', 'http://www.deeploai.com/notfound.php']\n",
    "for url in url_list:\n",
    "    r=requests.get(url)\n",
    "    print('{} : {}'.format(url,r.status_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 1:__\n",
    "- What is the reason of Error-403? Is it possible to surpass this error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3.2. Header Editing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'http://worldagnetwork.com/'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'}\n",
    "r=requests.get(url, headers=headers)\n",
    "print('Request via normal browser: {}'.format(r.status_code))\n",
    "mobile_headers = {'User-Agent' : 'Mozilla/5.0 (iPhone; CPU iPhone OS 9_1 like Mac OS X) AppleWebKit/601.1.46 (KHTML, like Gecko) Version/9.0 Mobile/13B137 Safari/601.1'}\n",
    "r=requests.get(url, headers=mobile_headers)\n",
    "print('Request via mobile browser: {}'.format(r.status_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Editing URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.1. Adding Standard Parameters To a URL__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url='http://www.domainname.com/'\n",
    "payload = {'key1': 'value1', 'key2': 'value2'}\n",
    "r=requests.get(url, params=payload)\n",
    "print(r.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question-2:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can you edit the URLs that every URL has a only one key-value pairs from payload?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url='http://www.domainname.com/'\n",
    "payload = {'key1': 'value1', 'key2': 'value2'}\n",
    "for k,v in payload.items():\n",
    "    r=requests.get(url, params={k:v})\n",
    "    print(r.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.2. Google Search URL Parameters__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic URL: http://www.google.com/search?\n",
    "\n",
    "- Single Keyword Query: __q=__deeploai\n",
    "\n",
    "- Multiple Keyword Query: __q=__deeploai+netherlands\n",
    "\n",
    "- Keyword(s) in Quotes: __as_epq=__deeploai+netherlands\n",
    "\n",
    "- Limit the Result Number: __num=__100\n",
    "\n",
    "- File Type: __as_filetype=__pdf\n",
    "\n",
    "- Search in Spesific Web Site: __as_sitesearch=__deeploai.com\n",
    "\n",
    "- Search in Spesific Time Duration: \n",
    "\n",
    "    - The previous 24 hours : __as_qdr=d__\n",
    "    - The previous seven days : __as_qdr=w__\n",
    "    - The previous month : __as_qdr=m__\n",
    "    - The previous 3 month :__as_qdr=m3__\n",
    "    - Past year: __as_qdr=y__\n",
    "\n",
    "[Detail For Google URL Parameters](https://moz.com/blog/the-ultimate-guide-to-the-google-search-parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url='http://www.google.com/search?'\n",
    "payload = {'q': 'deeploai', 'as_qdr': 'w'}\n",
    "r=requests.get(url, params=payload)\n",
    "print(r.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.3. Adding Variable-Parameters To a URL__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get URLs with keywords from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "keywords=['data+scientist','data+engineer','data+analist']\n",
    "for keyword in keywords:\n",
    "    url='https://www.indeed.nl/job?q={}'.format(keyword)\n",
    "    r=requests.get(url)\n",
    "    print(r.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or\n",
    "import requests\n",
    "keywords=['data+scientist','data+engineer','data+analist']\n",
    "for n in range(len(keywords)):\n",
    "    url='https://www.indeed.nl/job?q={}'.format(keywords[n])\n",
    "    r=requests.get(url)\n",
    "    print(r.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get URLs with page numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "keywords=['pandas']\n",
    "for n in range(1,11):\n",
    "    url='https://stackoverflow.com/questions/tagged/{}?tab=newest&page={}'.format(keywords[0],n)\n",
    "    r=requests.get(url)\n",
    "    print(r.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4.4. Finding The Parameters In URLs__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the parameters of a URL for a query for 'https://www.internationalparceltracking.com'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the parameters from the browser Developer Console:\n",
    "- Developer Console >> Network Tab >> Headers Tab >> Query String Parameters (For Chrome)\n",
    "- Web Developer >> Network Tab >> Params Tab (For Firefox)\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/msklc/crash_web_scrape_notes/blob/master/images/browser_DeveloperConsole.jpg?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question-3:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the URL of barcode with '123456789', postal code with '99999' from France at PostNL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url='https://www.internationalparceltracking.com/api/shipment?barcode=123456789&checkIfValid=true&country=FR&language=en&postalCode=99999'\n",
    "r=requests.get(url)\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Gathering Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.1. General Content__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of municipalities of the Netherlands at wikipedia\n",
    "import requests\n",
    "url='https://en.wikipedia.org/wiki/List_of_municipalities_of_the_Netherlands'\n",
    "r=requests.get(url)\n",
    "r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__It is not easy to gathering relevant data!!!__\n",
    "\n",
    "So, we prefer to use BeautifulSoup to get the relevant data easily;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url='https://en.wikipedia.org/wiki/List_of_municipalities_of_the_Netherlands'\n",
    "r=requests.get(url)\n",
    "soup = BeautifulSoup(r.text,'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.2. Specific Content__\n",
    "\n",
    "We need to understand the defination of __tags__ in HTML\n",
    "\n",
    "<img src=\"https://github.com/msklc/crash_web_scrape_notes/blob/master/images/tags_of_HTML.jpg?raw=true\">\n",
    "\n",
    "| Tag | Description |\n",
    "| --- | --- |\n",
    "| __div__ | Division/Section in a page | \n",
    "| __table__ | Defines a table | \n",
    "| __th__ | Defines a header cell in a table | \n",
    "| __tr__ | Defines a row in a table | \n",
    "| __td__ | Defines a cell in a table | \t\n",
    "| __span__ | Generic inline container |\n",
    "| __a__ | Defines a hyperlink |\n",
    "| __ul__ | Defines an unordered list |\n",
    "| __li__ | Defines each list item |\n",
    "\n",
    "<img src=\"https://github.com/msklc/crash_web_scrape_notes/blob/master/images/browser_inspector.jpg?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example:__\n",
    "\n",
    "Find all hyperlinks in previous page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url='https://en.wikipedia.org/wiki/List_of_municipalities_of_the_Netherlands'\n",
    "r=requests.get(url)\n",
    "soup = BeautifulSoup(r.text,'html.parser')\n",
    "\n",
    "links = soup.find_all('a')\n",
    "print('Total hyperlinks in the page:',len(links))\n",
    "#print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example:__\n",
    "\n",
    "Find all __http__ hyperlinks in previous page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url='https://en.wikipedia.org/wiki/List_of_municipalities_of_the_Netherlands'\n",
    "r=requests.get(url)\n",
    "soup = BeautifulSoup(r.text,'html.parser')\n",
    "\n",
    "links = soup.find_all('a')\n",
    "\n",
    "for link in links:\n",
    "    tag=link.get('href')\n",
    "    try:\n",
    "        print(tag) if 'http' in tag else None\n",
    "    except:\n",
    "        None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question-3:__\n",
    "\n",
    "Why we need to use __try__ in previous example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example:__\n",
    "\n",
    "Gather the specific string: __25,445__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'25,445'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find vs find_all\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url='https://en.wikipedia.org/wiki/List_of_municipalities_of_the_Netherlands'\n",
    "r=requests.get(url)\n",
    "soup = BeautifulSoup(r.text,'html.parser')\n",
    "\n",
    "tag=soup.find('span',{'data-sort-value':\"7004254450000000000â™ \"}).text\n",
    "tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "__Example:__\n",
    "    \n",
    "(population example above can be often subjected to update, so this one could be much more surviving...)\n",
    "    \n",
    "Gather the specific string:__'Aa en Hunze'__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aa en Hunze'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#or we can pull another string\n",
    "#find vs find_all\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url='https://en.wikipedia.org/wiki/List_of_municipalities_of_the_Netherlands'\n",
    "r=requests.get(url)\n",
    "soup = BeautifulSoup(r.text,'html.parser')\n",
    "\n",
    "tag=soup.find('th',{'scope':\"row\"}).find(\"a\").text\n",
    "tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example:__\n",
    "\n",
    "Gather the all Municipalities name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url='https://en.wikipedia.org/wiki/List_of_municipalities_of_the_Netherlands'\n",
    "r=requests.get(url)\n",
    "soup = BeautifulSoup(r.text,'html.parser')\n",
    "\n",
    "table=soup.find('table',{'class':'wikitable plainrowheaders sortable'})\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url='https://en.wikipedia.org/wiki/List_of_municipalities_of_the_Netherlands'\n",
    "r=requests.get(url)\n",
    "soup = BeautifulSoup(r.text,'html.parser')\n",
    "\n",
    "table=soup.find('table',{'class':'wikitable plainrowheaders sortable'})\n",
    "\n",
    "municipalities=table.find_all('th')\n",
    "\n",
    "for row in municipalities:\n",
    "    print(row.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.3. CAPSTONE-1:__\n",
    "\n",
    "- Gather __all detail data__ of Netherlands municipalities from [wikipedia](https://en.wikipedia.org/wiki/List_of_municipalities_of_the_Netherlands)\n",
    "- Save them to CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url='https://en.wikipedia.org/wiki/List_of_municipalities_of_the_Netherlands'\n",
    "r=requests.get(url)\n",
    "soup = BeautifulSoup(r.text,'html.parser')\n",
    "\n",
    "table=soup.find('table',{'class':'wikitable plainrowheaders sortable'})\n",
    "rows=table.find_all('tr')[1:]\n",
    "\n",
    "for row in rows:\n",
    "    municipality=row.find('th').text.strip()\n",
    "    print(municipality)\n",
    "    cbs_code=row.find_all('td')[0].text.strip()\n",
    "    print(cbs_code)\n",
    "    province=row.find_all('td')[1].text.strip()\n",
    "    print(province)\n",
    "    population=row.find_all('td')[2].text.strip()\n",
    "    print(population)\n",
    "    pop_density=row.find_all('td')[3].text.strip()\n",
    "    print(pop_density)\n",
    "    land_area=row.find_all('td')[4].text.strip()\n",
    "    print(land_area)\n",
    "    print('=======')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5.4. BONUS: Scrape Tables with Pandas__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url='https://en.wikipedia.org/wiki/List_of_municipalities_of_the_Netherlands'\n",
    "table=pd.read_html(url,match='Aa en Hunze')\n",
    "table[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Some Other Usefull Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6.1. Adding Cookies To Requests__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question-4:__\n",
    "\n",
    "- What is the reason of Error-403? Is it possible to surpass this error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the cookies from the browser Developer Console:\n",
    "- Developer Console >> Application Tab >> Cookies Tab (For Chrome)\n",
    "- Web Developer >> Storage Tab >> Cookies Tab (For Firefox)\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/msklc/crash_web_scrape_notes/blob/master/images/sessionID_chrome.jpg?raw=true\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url='https://www.deeploai.com'\n",
    "cookie = {'PHPSESSID': 'XXXX','ZHE':'YYY'}\n",
    "r = requests.post(url, cookies=cookie)\n",
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6.2. Scraping via Proxies__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url='https://www.deeploai.com'\n",
    "proxies = {'http': '110.74.209.202:51491'} #free proxy adress\n",
    "\n",
    "r = requests.get(url, proxies=proxies)\n",
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. CAPSTONE-2\n",
    "\n",
    "- Scrape all Angola movies from IMDB\n",
    "    - Movie names\n",
    "    - Produced Year\n",
    "    - IMDB Rank\n",
    "- Save them to CSV file\n",
    "\n",
    "(Hint: Use IMDB Advance search page https://www.imdb.com/search/title/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good Luck!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
